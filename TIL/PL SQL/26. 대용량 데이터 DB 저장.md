> 총 17개 csv파일
> 전국 소상공인 가게에 대한 정보가 있음.
> 데이터 크기 매우 많음, 첫번째 파일 row수 107,833 
> 두번째, 107833의 6배 
# 1. saveAll() 
> 수행 시간     : 
> 데이터 개수 : 

insert 전에 select 하는 이유
select해서 존재하는 데이터는 update하고, 없으면 insert하기 때문.
속도 저하 원인이기도 하다.
``` java
public void saveLogic() throws IOException {  
 
        File directory = new File(getClass().getClassLoader().getResource("csvFiles").getFile());  
        if(!directory.exists() && !directory.isFile()){  
            throw new FileNotFoundException("File Not Found");  
        }  
        List<StoreInfo> list = new ArrayList<>();  
        File[] files = directory.listFiles();  
        for(int i=0; i<files.length; i++){  
            BufferedReader br = new BufferedReader(new java.io.FileReader(files[i]));  
            while(true){  
                String data = br.readLine();  
                if(data == null){  
                    break;  
                }  
                data=data.replaceAll("\"", "" );  
                StoreInfo storeInfo = new StoreInfo(data.split(","));  
                list.add(storeInfo);  
            }  
            storeInfoRepository.saveAll(list);  
            System.out.println(list.size());  
            list = new ArrayList<>();  
        }  
    }  
}
```
데이터 수가 너무 많아서 메모리 초과가 발생할 가능성 존재.
2번째 파일 약 60만개가 list에 저장되기 때문..
60만번 select 후 insert

데이터 저장이 끝나질 않음. 시간이 매우 오래걸림

# 2. jpa batch & persist()



# 3. java -> csv -> db -> pl/sql collection & FORALL 이용하여 저장
> 수행 시간      : 
> 데이터 개수  : 



